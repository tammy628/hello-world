{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttolofari/hello-world/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4HU7W1fuSWBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoCOQTOPSgSC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -cq \"https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9V8wKk4HYPuT",
        "colab_type": "code",
        "outputId": "ca3c123d-ce84-4721-a153-555fd46535c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  text8.zip\n",
            "replace text8? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MLjWOfGTSQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "\n",
        "with open('text8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pinr72LYjH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tmdbS5OYmTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing the data usin\n",
        "\n",
        "words = utils.preprocess(text)\n",
        "print(words[:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YmWBbbjasSm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print some stats about this word data\n",
        "print(\"Total words in text: {}\".format(len(words)))\n",
        "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1gsFOcEvjo2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating dictionaries to convert words to integers and back again (integers to words)\n",
        "\n",
        "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
        "\n",
        "int_words = [vocab_to_int[word] for word in words]\n",
        "\n",
        "print(int_words[:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwFQXI3TkP-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Subsampling\n",
        "\n",
        "# Discarding words given that they surpass the \n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "threshold = 1e-5\n",
        "word_counts = Counter(int_words)\n",
        "\n",
        "\n",
        "print(list(word_counts.items())[0]) # Showing how many times each word appears in the text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mckHc_aPlX5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_count = len(int_words)\n",
        "\n",
        "\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "# discard some frequent words, according to the subsampling equation\n",
        "# create a new list of words for training\n",
        "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "print(train_words[:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gpe4PpNKpKZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making Batches\n",
        "\n",
        "#With the skip-gram architecture, for each word in he text, we want to define a surrounding context and grab all the words in a window around that word, with size C\n",
        "#We usually give less weigh to distant words because distant words are usually less related to the current word.\n",
        "\n",
        "\n",
        "def get_target(words, idx, window_size=5):\n",
        "    '''Geat a list of words in a window around an index'''\n",
        "    \n",
        "    R = np.random.randint(1, window_size + 1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    target_words = words[start : idx] + words[idx+1 : stop+1]\n",
        "    \n",
        "    return list(target_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8O7_HFQ292H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing the code\n",
        "\n",
        "int_text = [i for i in range(10)]\n",
        "print('Input: ', int_text)\n",
        "idx = 5 # word index of interest\n",
        "\n",
        "target = get_target(int_text, idx = idx, window_size = 5)\n",
        "print('Target: ', target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbBOB-Ud3IKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generating Batches\n",
        "\n",
        "def get_batches(words, batch_size, window_size = 5):\n",
        "    '''Create a generator of word batches as a tuple (inputs, targets)'''\n",
        "    n_batches = len(words)//batch_size\n",
        "    \n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx + batch_size]\n",
        "        \n",
        "        for ii in range(len(batch)):\n",
        "          batch_x = batch[ii]\n",
        "          batch_y = get_target(batch, ii, window_size)\n",
        "          y.extend(batch_y)\n",
        "          x.extend([batch_x]*len(batch_y))\n",
        "          yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aoGys6OE6A-W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_text = [i for i in range(20)]\n",
        "x, y = next(get_batches(int_text, batch_size = 4, window_size = 5))\n",
        "\n",
        "print('x\\n', x)\n",
        "print('y\\n', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_HFKFev6Td3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Building the Graph\n",
        "\n",
        "\n",
        "# This uses the cosine similarity to caclculate how similar words are to each other.\n",
        "\n",
        "\n",
        "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
        "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
        "        Here, embedding should be a PyTorch embedding module.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Here we're calculating the cosine similarity between some random words and \n",
        "    # our embedding vectors. With the similarities, we can look at what words are\n",
        "    # close to our random words.\n",
        "    \n",
        "    # sim = (a . b) / |a||b|\n",
        "    \n",
        "    embed_vectors = embedding.weight\n",
        "    \n",
        "    # magnitude of embedding vectors, |b|\n",
        "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
        "    \n",
        "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
        "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
        "    valid_examples = np.append(valid_examples,\n",
        "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
        "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
        "    \n",
        "    valid_vectors = embedding(valid_examples)\n",
        "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
        "        \n",
        "    return valid_examples, similarities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49fZs8ASNf54",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define and train the SkipGram model.\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wejb_MPpOR-J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
        "        self.output = nn.Linear(n_embed, n_vocab)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        scores = self.output(x)\n",
        "        log_ps = self.log_softmax(scores)\n",
        "        \n",
        "        return log_ps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xl48VOBTOe1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "\n",
        "# check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "embedding_dim=300 # you can change, if you want\n",
        "\n",
        "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "print_every = 500\n",
        "steps = 0\n",
        "epochs = 5\n",
        "\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    \n",
        "    # get input and target batches\n",
        "    for inputs, targets in get_batches(train_words, 512):\n",
        "        steps += 1\n",
        "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        log_ps = model(inputs)\n",
        "        loss = criterion(log_ps, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % print_every == 0:                  \n",
        "            # getting examples and similarities      \n",
        "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
        "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
        "            \n",
        "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
        "            for ii, valid_idx in enumerate(valid_examples):\n",
        "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
        "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
        "            print(\"...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-7l3JQtOnnZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}